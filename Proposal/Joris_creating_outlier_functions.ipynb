{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the files to be shared among all notebooks \n",
    "import utils\n",
    "import preprocessing\n",
    "import data_visualization\n",
    "import feature_engineering\n",
    "from ML_algorithms import *\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering.drop_useless_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.to_dtype_object(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.encode_days_as_costumer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.impute_income_KNN(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing outliers using the IQR method with 2 quartiles would lead to a change of data size:  -0.004910714285714286\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing.outlier_IQR(df, [\"Year_Birth\",\"Income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
       "       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n",
       "       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
       "       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
       "       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n",
       "       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n",
       "       'AcceptedCmp2', 'Complain', 'Response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing.Binning_Features(df, \"Income\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntWines\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntFruits\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntMeatProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntFishProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntSweetProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntGoldProds\", n_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.encode_education(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = utils.X_y_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/2_Semester/Machine Learning/MLProjects/Proposal/ML_algorithms.py:66: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  model.add(layers.Dense(1, activation=\"sigmoid\", init=init))\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"KerasNN_not_fitted\" : KerasNN_not_fitted(),\n",
    "    \"GaussianNB\" : GaussianNB(),\n",
    "    \"MultinomialNB\" : MultinomialNB(),\n",
    "    \"ComplementNB\" : ComplementNB(),\n",
    "    \"SVC\" : SVC(), \n",
    "    \"LinearSVC\" : LinearSVC(),\n",
    "    \"LogisticRegression\" : LogisticRegression(),\n",
    "    \"SGDClassifier\" : SGDClassifier(),\n",
    "    \"KNeighborsClassifier\" : KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\" : DecisionTreeClassifier(criterion=\"gini\", class_weight=None),\n",
    "    \"XGBClassifier\" : XGBClassifier(colsample_by_tree=0.1,\n",
    "                                  learning_rate=0.89,\n",
    "                                  max_depth=8,\n",
    "                               n_estimators=10000,\n",
    "                                  eval_metric=\"auc\",                                \n",
    "                                  n_jobs=-1, silent=0, verbose=0),\n",
    "    \"MLPClassifier\" : MLPClassifier(hidden_layer_sizes=(10), solver = \"lbfgs\", max_iter=1000, random_state=42),\n",
    "    \"LinearRegression\" : LinearRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/2_Semester/Machine Learning/MLProjects/Proposal/ML_algorithms.py:66: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  model.add(layers.Dense(1, activation=\"sigmoid\", init=init))\n"
     ]
    }
   ],
   "source": [
    "models = {\"KerasNN_not_fitted\" : KerasNN_not_fitted()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Graduation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9b1844b3e333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCross_Val_Models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_technique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/2_Semester/Machine Learning/MLProjects/Proposal/utils.py\u001b[0m in \u001b[0;36mCross_Val_Models\u001b[0;34m(models, X, y, scaler, n_splits, sampling_technique)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_average_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampling_technique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_technique\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/2_Semester/Machine Learning/MLProjects/Proposal/utils.py\u001b[0m in \u001b[0;36mcross_validation_average_results\u001b[0;34m(model, X, y, n_splits, sampling_technique, scaler, **model_kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    360\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m    361\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Graduation'"
     ]
    }
   ],
   "source": [
    "utils.Cross_Val_Models(models, X, y, sampling_technique=None, scaler=scaler)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy import stats\n",
    "\n",
    "def Binning_Features(df, feature=\"Income\", n_bins=10, strategy=\"quantile\", cont_tab=False):\n",
    "    target = \"Response\"\n",
    "\n",
    "    bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "    mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "\n",
    "    if cont_tab == True:\n",
    "        obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "        df[feature] = mnt_bin\n",
    "        print(obs_cont_tab)\n",
    "        \n",
    "    df[feature] = mnt_bin\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\"MntWines\",\"MntFruits\",\"Income\",'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
    "       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
    "       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "\n",
    "for feature in features:\n",
    "    Binning_features(df, feature=\"Income\", n_bins=10, cont_tab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Income_bin\"] = mnt_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cont_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) obtain the observed contingency table\n",
    "feature, target = \"Education\", \"Response\"\n",
    "df_rec = df[[feature, target]]\n",
    "\n",
    "obs_cont_tab = pd.crosstab(df_rec[feature], df_rec[target], margins = True)\n",
    "obs_cont_tab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) calculate the expected contingency table, assuming there is no association\n",
    "exp_cont_tab = obs_cont_tab.copy()\n",
    "\n",
    "n_r = exp_cont_tab.iloc[:-1, -1]\n",
    "N = exp_cont_tab.iloc[-1, -1]\n",
    "for c in range(obs_cont_tab.shape[1]-1):\n",
    "    n_c = exp_cont_tab.iloc[-1, c]\n",
    "    exp_cont_tab.iloc[:-1, c] = np.divide(np.multiply(n_c, n_r), N)\n",
    "\n",
    "# visually compare both tables, side by side\n",
    "pd.concat([obs_cont_tab.iloc[:, :-1], exp_cont_tab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) compute the test statistic as measure of dissimilarity between the expected and observed tables\n",
    "obs, exp = [], []\n",
    "for c in range(obs_cont_tab.shape[1]-1):\n",
    "    exp.append(exp_cont_tab.iloc[:-1, c].values)\n",
    "    obs.append(obs_cont_tab.iloc[:-1, c].values)\n",
    "    \n",
    "chi_squared_stat = np.sum(np.divide(np.power(np.subtract(obs, exp), 2), exp))\n",
    "print('Chi-squared test statistic: {0:.2f}'.format(chi_squared_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "chisq = stats.chi2_contingency(obs_cont_tab.iloc[:-1, [0, 1]].values)[0:2]\n",
    "print(\"Test statistic: {0:.2f}, p-value: {1:.4f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bar_charts_categorical(df, feature, target):\n",
    "    cont_tab = pd.crosstab(df[feature], df[target], margins = True)\n",
    "    categories = cont_tab.index[:-1]\n",
    "        \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, \n",
    "                 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, \n",
    "                 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"red\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "    \n",
    "    mean_target = 1-df[target].mean()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"red\")\n",
    "    plt.plot([.5, len(categories)+.5], [mean_target, mean_target],'--', lw=2, color=\"black\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i in df:\n",
    "    bar_charts_categorical(df, i, \"Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# 1) equal width binning of a continuous feature\n",
    "feature, n_bins = \"Income\", 10\n",
    "bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "print(\"Parameters of discretizer: \", bindisc.get_params())\n",
    "print(\"Thresholds for {} bins of {}: \".format(n_bins, feature), bindisc.bin_edges_)\n",
    "\n",
    "# 2) generate a contingency table, required for the Chi-Squared test\n",
    "obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "\n",
    "# 3) compute Chi-Squared test for binned feature\n",
    "chisq=stats.chi2_contingency(obs_cont_tab.values)[0:2]\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) equal width binning of a continuous feature\n",
    "feature, n_bins = \"Income\", 10\n",
    "bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "print(\"Parameters of discretizer: \", bindisc.get_params())\n",
    "print(\"Thresholds for {} bins of {}: \".format(n_bins, feature), bindisc.bin_edges_)\n",
    "\n",
    "# 2) generate a contingency table, required for the Chi-Squared test\n",
    "obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "\n",
    "# 3) compute Chi-Squared test for binned feature\n",
    "chisq=stats.chi2_contingency(obs_cont_tab.values)[0:2]\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bar_charts_continuous(df, feature, target, n_bins=10, binning_strategy=\"uniform\", chi_sq=False):\n",
    "    bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n",
    "                               strategy=binning_strategy)\n",
    "    feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "    feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n",
    "    \n",
    "    cont_tab = pd.crosstab(feature_bin, df[target], margins = True)\n",
    "    categories = cont_tab.index[:-1].astype(str) \n",
    "    \n",
    "    if chi_sq:\n",
    "        chisq = stats.chi2_contingency(cont_tab.values)[0:2]       \n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, \n",
    "                 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, \n",
    "                 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"red\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "    \n",
    "    mean_target = 1-df[target].mean()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"red\")\n",
    "    plt.plot([-.5, len(categories)], [mean_target, mean_target],'--', lw=2, color=\"black\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "    plt.show()\n",
    "    \n",
    "    if chi_sq:\n",
    "        return chisq\n",
    "\n",
    "chi_sq=bar_charts_continuous(df, \"Income\", \"Response\", 10, \"uniform\", True)\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chi_sq[0], chi_sq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, target = \"Income\", \"Response\"\n",
    "df_smd = df[[feature, target]]\n",
    "\n",
    "mean_feature = df_smd[feature].mean()\n",
    "df_smd[feature+\"_SMD\"] = (df_smd[feature].sub(mean_feature)).div(mean_feature)\n",
    "\n",
    "smd_summary = df_smd.groupby([target]).agg({feature+\"_SMD\": \"mean\", feature: \"mean\"})  \n",
    "display(smd_summary)\n",
    "print(\"Average {0}: {1:.2f}, SMD of {2}: {3:.2f}\".format(feature, mean_feature, feature, smd_summary[feature+\"_SMD\"].abs().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the bar chart\n",
    "plt.bar(x = smd_summary.index, height=smd_summary[feature+\"_SMD\"], color=\"gray\")\n",
    "plt.title(\"SMD of \" + feature)\n",
    "plt.xlabel(target)\n",
    "plt.ylabel(\"$SMD$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) intermediary step: compute SMD0, SMD1 and SMD for every continuous feature\n",
    "num_list = df.select_dtypes(include=[\"number\"]).drop([\"Response\"], axis=1).columns\n",
    "target = \"Response\"\n",
    "\n",
    "smd_dict = {\"smd0\": [], \"smd1\": [], \"smd\": []}\n",
    "for feature in num_list:\n",
    "    df_smd = df[[feature, target]]\n",
    "    mean_feature = df_smd[feature].mean()\n",
    "    df_smd[feature] = (df_smd[feature].sub(mean_feature)).div(mean_feature) \n",
    "    smd_summary = df_smd.groupby([target]).agg({feature: \"mean\"})  \n",
    "    smd_dict[\"smd0\"].append(smd_summary.iloc[0, 0])\n",
    "    smd_dict[\"smd1\"].append(smd_summary.iloc[1, 0])\n",
    "    smd_dict[\"smd\"].append(smd_summary[feature].abs().sum())\n",
    "\n",
    "smd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) intermediary step: plot SMD0 (gray), SMD1 (red) ordered by SMD\n",
    "import operator\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.DataFrame(smd_dict, index=num_list).sort_values(\"smd\", ascending=False)\n",
    "colors, groups = (\"gray\", \"red\"), (\"non-respondents\", \"respondents\")\n",
    "g0, g1 = (df1.index, df1.iloc[:, 0]), (df1.index, df1.iloc[:, 1])\n",
    "data = (g0, g1)\n",
    "\n",
    "fig = plt.figure(figsize=(len(df1.index)+1, 4))\n",
    "ax = fig.add_subplot(1, 1, 1, facecolor = \"1.0\")\n",
    "for data, color, group in zip(data, colors, groups):    \n",
    "    X, y = data\n",
    "    ax.scatter(x=X, y=y.values, s=80, marker=\"s\", label=group, c=color)\n",
    "\n",
    "ax.set_title(\"Scaled Mean Deviation (SMD)\")\n",
    "ax.set_xlabel(\"Continuous features\")\n",
    "ax.set_ylabel(\"SMD\")\n",
    "ax.legend()\n",
    "plt.xticks(fontsize=9, rotation=60)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smd01_plot(df, nfeature_list, target, return_df_smd=True):  \n",
    "    smd_dict = {\"smd0\": [], \"smd1\": [], \"smd\": []}\n",
    "    for feature in num_list:\n",
    "        df_smd = df[[feature, target]]\n",
    "        mean_feature = df_smd[feature].mean()\n",
    "        df_smd[feature] = (df_smd[feature].sub(mean_feature)).div(mean_feature) \n",
    "        df_smd = df_smd.groupby([target]).agg({feature: \"mean\"})  \n",
    "        smd_dict[\"smd0\"].append(df_smd.iloc[0, 0])\n",
    "        smd_dict[\"smd1\"].append(df_smd.iloc[1, 0])\n",
    "        smd_dict[\"smd\"].append(df_smd[feature].abs().sum())\n",
    "\n",
    "    df_smd = pd.DataFrame(smd_dict, index=num_list).sort_values(\"smd\", ascending=False)\n",
    "    colors, groups = (\"gray\", \"red\"), (\"non-respondents\", \"respondents\")\n",
    "    g0, g1 = (df_smd.index, df_smd.iloc[:, 0]), (df_smd.index, df_smd.iloc[:, 1])\n",
    "    data = (g0, g1)\n",
    "\n",
    "    fig = plt.figure(figsize=(len(df_smd.index)+1, 4))\n",
    "    ax = fig.add_subplot(1, 1, 1, facecolor = \"1.0\")\n",
    "    for data, color, group in zip(data, colors, groups):    \n",
    "        X, y = data\n",
    "        ax.scatter(x=X, y=y.values, s=80, marker=\"s\", label=group, c=color)\n",
    "\n",
    "    ax.set_title(\"Scaled Mean Deviation (SMD)\")\n",
    "    ax.set_xlabel(\"Continuous features\")\n",
    "    ax.set_ylabel(\"SMD\")\n",
    "    ax.legend()\n",
    "    plt.xticks(fontsize=9, rotation=60)    \n",
    "    plt.show()\n",
    "    \n",
    "    if return_df_smd:\n",
    "        return df_smd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smd = smd01_plot(df, num_list, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def chisq_ranker(df, target,continuous_flist = list(df.select_dtypes(include=[\"number\"]).drop([\"Response\"], axis=1).columns),categorical_flist = list(df.select_dtypes(include=[\"object\"]).columns), n_bins=10, binning_strategy=\"uniform\"):\n",
    "    \"\"\"\n",
    "    Input the dataframe and the target, you can also choose the continuous variables you want to include from the dataframe but a default has been set.\n",
    "    The same goes for the categorical variables.\n",
    "    The output that will be given is the ranking of the features using the Chi-Squared test\n",
    "    \"\"\"\n",
    "    chisq_dict = {}\n",
    "    if  continuous_flist:\n",
    "        bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n",
    "                               strategy=binning_strategy)\n",
    "        for feature in continuous_flist:            \n",
    "            feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n",
    "            cont_tab = pd.crosstab(feature_bin, df[target], margins = False)\n",
    "            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2] \n",
    "    if  categorical_flist:\n",
    "        for feature in categorical_flist:  \n",
    "            cont_tab = pd.crosstab(df[feature], df[target], margins = False)          \n",
    "            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2]\n",
    "            \n",
    "    df_chisq_rank = pd.DataFrame(chisq_rank, index=[\"Chi-Squared\", \"p-value\"]).transpose()\n",
    "    df_chisq_rank.sort_values(\"Chi-Squared\", ascending=False, inplace=True)\n",
    "    df_chisq_rank[\"valid\"]=df_chisq_rank[\"p-value\"]<=0.05\n",
    "    df_chisq_rank\n",
    "    return df_chisq_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chisq_rank = chisq_ranker(df, target)\n",
    "df_chisq_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisq_ranker_plot(df_chisq_rank):\n",
    "    \"\"\"\n",
    "    The Chi-Square ranker plot takes the output of the Chi-Squared ranker function and creates a plot out of the inputted dataframe.\n",
    "    \"\"\"\n",
    "    colors = {True: 'gray', False: 'lightgray'}\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    df_chisq_rank['Chi-Squared'].plot(ax=ax, kind='bar', color=[colors[i] for i in df_chisq_rank['valid']])\n",
    "    ax.set_title(\"Features worth by Chi-Squared test statistic\")\n",
    "    ax.set_xlabel(\"Input features\")\n",
    "    ax.set_ylabel(\"Test statistic\")\n",
    "    ax.set_xticklabels(df_chisq_rank.index, rotation=70)\n",
    "    ax.legend([\"p-value<=0.5\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq_ranker_plot(df_chisq_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_income_KNN(df):\n",
    "    dataframe = df.copy()\n",
    "    dataframe_c = dataframe.dropna().select_dtypes(include=[\"number\"]).drop([\"Response\"], axis = 1)\n",
    "    dataframe_i = dataframe[pd.isnull(dataframe).any(axis=1)].select_dtypes(include=[\"number\"]).drop([\"Response\"], axis = 1)\n",
    "    clf = KNeighborsClassifier(3, weights='uniform', metric = 'euclidean')\n",
    "    trained_model = clf.fit(dataframe_c.drop([\"Income\"],axis=1), dataframe_c.loc[:,'Income'])\n",
    "    imputed_values = trained_model.predict(dataframe_i.drop([\"Income\"], axis=1))\n",
    "    #print(imputed_values)\n",
    "    dataframe_i[\"Income\"] = imputed_values\n",
    "    dataframe_new = pd.concat([df_i, df_c])\n",
    "    dataframe_new = dataframe_new.sort_index()\n",
    "    dataframe[\"Income\"] = dataframe_new[\"Income\"]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_IQR(df, columns=[\"Year_Birth\",\"Income\"]):\n",
    "    \"\"\"\n",
    "    outlier deletetion using the IQR, you can choose the variables you want to delete the outliers for by selecting the columns. The default is Year_Birth & Income. Also you can change the quantile values.\n",
    "    \"\"\"\n",
    "    dataframe = df.copy()\n",
    "    Q1 = dataframe[columns].quantile(0.25)\n",
    "    Q3 = dataframe[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    #print(IQR)\n",
    "    dataframe[columns] = dataframe[columns][~((dataframe < (Q1 - 2 * IQR)) |(dataframe > (Q3 + 2 * IQR))).any(axis=1)]\n",
    "    dataframe = dataframe.dropna()\n",
    "    print('Removing outliers using the IQR method with 2 quartiles would lead to a change of data size: ',(dataframe.shape[0] -df.shape[0]) /df.shape[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_ZSCORE(df, columns=[\"Year_Birth\", \"Income\"], threshold=3):\n",
    "    \"\"\"\n",
    "    outlier deletion using the Zscore, you can choose which columns you want to apply it on and you can choose which threshold you want to use.\n",
    "    \"\"\"\n",
    "    dataframe = df.copy()\n",
    "    columns_zscore = []\n",
    "    for i in dataframe[columns]:\n",
    "        i_zscore = i + \"_zscore\"\n",
    "        columns_zscore.append(str(i_zscore))\n",
    "        \n",
    "        dataframe[i_zscore] = (dataframe[i] - dataframe[i].mean())/df[i].std(ddof=0)\n",
    "    for i in dataframe[columns_zscore]:\n",
    "        dataframe = dataframe[(dataframe[i] < threshold) & (dataframe[i] > -threshold)]\n",
    "    dataframe = dataframe.drop(columns_zscore, axis=1)\n",
    "    print('Removing outliers using the ZSCORES method with a threshold of 3 would lead to a change of data size: ',(dataframe.shape[0] -df.shape[0]) /df.shape[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_IQR(df,[\"Year_Birth\",\"Income\",\"MntSweetProducts\",\"MntMeatProducts\",\"MntGoldProds\"]).head()\n",
    "\n",
    "outlier_ZSCORE(df,[\"Year_Birth\",\"Income\",\"MntSweetProducts\",\"MntMeatProducts\",\"MntGoldProds\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def Min_Max_Train(X_train, X_test):    \n",
    "    scaler = MinMaxScaler()\n",
    "    # Only fit the training data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def KerasNN1(X_train, X_test, y_train, y_test, n_layers=4, optimizer=\"rmsprop\", loss=\"binary_crossentropy\", init=\"uniform\", metrics=[\"accuracy\"]):\n",
    "    \"\"\"\n",
    "    Keras Neural Network, define the amount of layers you want, which optimizer you want to use and which loss function you want to apply.\n",
    "    \"\"\" \n",
    "    np.random.seed(42)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(6, activation=\"relu\", input_dim=len(df.columns) -1))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\", init=init))\n",
    "    model.compile(optimizer, loss, metrics=metrics)\n",
    "    \n",
    "    initial_weights = model.get_weights()\n",
    "    \n",
    "    shuffle_weights(model, initial_weights)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_evaluation(model):\n",
    "    y_predicted = model.predict(X_test)\n",
    "    threshold = utils.max_threshold(y_predicted, y_test, threshold_range = (0.1, 0.99),iterations=10000, visualization=True)\n",
    "    y_pred = utils.predict_with_threshold(y_predicted,threshold)\n",
    "\n",
    "    print(\"Accuracy {:1.2f}\".format(utils.calculate_accuracy(y_pred, y_test)))\n",
    "    print(\"Area under the curve {:1.2f}\".format(utils.calculate_auc(y_pred, y_test)))\n",
    "    print(\"Precision {:1.2f}\".format(utils.calculate_precision_score(y_pred, y_test)))\n",
    "    print(\"Recall {:1.2f}\".format(utils.calculate_recall_score(y_pred, y_test)))\n",
    "    print(\"Profit Share {:1.2f}\".format(utils.profit_share(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def KerasNN(X_train, X_test, y_train, y_test, optimizer=\"rmsprop\", loss=\"binary_crossentropy\"):\n",
    "    \"\"\"\n",
    "    Try to variate in optimizers & loss functions\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    # Only fit the training data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # if we can pass a variable that indicates the amount of layers than it would be cool\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(6, activation=\"relu\", input_dim=30))\n",
    "    model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer, loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    threshold = utils.max_threshold(y_predicted, y_test, threshold_range = (0.1, 0.99),iterations=10000, visualization=True)\n",
    "    y_pred = utils.predict_with_threshold(y_predicted,threshold)\n",
    "    \n",
    "    print(\"Accuracy {:1.2f}\".format(utils.calculate_accuracy(y_pred, y_test)))\n",
    "    print(\"Area under the curve {:1.2f}\".format(utils.calculate_auc(y_pred, y_test)))\n",
    "    print(\"Precision {:1.2f}\".format(utils.calculate_precision_score(y_pred, y_test)))\n",
    "    print(\"Recall {:1.2f}\".format(utils.calculate_recall_score(y_pred, y_test)))\n",
    "    print(\"Profit Share {:1.2f}\".format(utils.profit_share(y_pred, y_test)))\n",
    "    return utils.profit_share(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
