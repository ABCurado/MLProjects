{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import all the files to be shared among all notebooks \n",
    "import utils\n",
    "import preprocessing\n",
    "import data_visualization\n",
    "import feature_engineering\n",
    "from ML_algorithms import *\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.joris_preprocessing_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Dt_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>...</th>\n",
       "      <th>Marital_Status_Together</th>\n",
       "      <th>Marital_Status_Widow</th>\n",
       "      <th>Education_2n Cycle</th>\n",
       "      <th>Education_Basic</th>\n",
       "      <th>Education_Graduation</th>\n",
       "      <th>Education_Master</th>\n",
       "      <th>Education_PhD</th>\n",
       "      <th>Responsiveness</th>\n",
       "      <th>ave_purchase</th>\n",
       "      <th>income_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1957</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>663</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.438914</td>\n",
       "      <td>0.027813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1954</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1965</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.487179</td>\n",
       "      <td>0.010836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381295</td>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.621118</td>\n",
       "      <td>0.007239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_Birth   Income  Kidhome  Teenhome  Dt_Customer  Recency  MntWines  \\\n",
       "0        1957  58138.0        0         0          663       58       635   \n",
       "1        1954  46344.0        1         1          113       38        11   \n",
       "2        1965  71613.0        0         0          312       26       426   \n",
       "3        1984  26646.0        1         0          139       26        11   \n",
       "4        1981  58293.0        1         0          161       94       173   \n",
       "\n",
       "   MntFruits  MntMeatProducts  MntFishProducts  ...  Marital_Status_Together  \\\n",
       "0         88              546              172  ...                        0   \n",
       "1          1                6                2  ...                        0   \n",
       "2         49              127              111  ...                        1   \n",
       "3          4               20               10  ...                        1   \n",
       "4         43              118               46  ...                        0   \n",
       "\n",
       "   Marital_Status_Widow  Education_2n Cycle  Education_Basic  \\\n",
       "0                     0                   0                0   \n",
       "1                     0                   0                0   \n",
       "2                     0                   0                0   \n",
       "3                     0                   0                0   \n",
       "4                     0                   0                0   \n",
       "\n",
       "   Education_Graduation  Education_Master  Education_PhD  Responsiveness  \\\n",
       "0                     1                 0              0             0.0   \n",
       "1                     1                 0              0             0.0   \n",
       "2                     1                 0              0             0.0   \n",
       "3                     1                 0              0             0.0   \n",
       "4                     0                 0              1             0.0   \n",
       "\n",
       "   ave_purchase  income_share  \n",
       "0      2.438914      0.027813  \n",
       "1      0.238938      0.000583  \n",
       "2      2.487179      0.010836  \n",
       "3      0.381295      0.001989  \n",
       "4      2.621118      0.007239  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Dt_Customer', 'Recency',\n",
       "       'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
       "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
       "       'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
       "       'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',\n",
       "       'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Response', 'Partner',\n",
       "       'income_housemember', 'Marital_Status_Alone', 'Marital_Status_Divorced',\n",
       "       'Marital_Status_Married', 'Marital_Status_Single',\n",
       "       'Marital_Status_Together', 'Marital_Status_Widow', 'Education_2n Cycle',\n",
       "       'Education_Basic', 'Education_Graduation', 'Education_Master',\n",
       "       'Education_PhD', 'Responsiveness', 'ave_purchase', 'income_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing.Binning_Features(df, \"Income\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntWines\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntFruits\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntMeatProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntFishProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntSweetProducts\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, \"MntGoldProds\", n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, 'Responsiveness', n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, 'ave_purchase', n_bins=5)\n",
    "df = preprocessing.Binning_Features(df, 'income_share', n_bins=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = utils.X_y_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"KerasNN_not_fitted\" : KerasNN_not_fitted(),\n",
    "    \"GaussianNB\" : GaussianNB(),\n",
    "    \"MultinomialNB\" : MultinomialNB(),\n",
    "    \"ComplementNB\" : ComplementNB(),\n",
    "    \"SVC\" : SVC(), \n",
    "    \"LinearSVC\" : LinearSVC(),\n",
    "    \"LogisticRegression\" : LogisticRegression(),\n",
    "    \"SGDClassifier\" : SGDClassifier(),\n",
    "    \"KNeighborsClassifier\" : KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\" : DecisionTreeClassifier(criterion=\"gini\", class_weight=None),\n",
    "    \"XGBClassifier\" : XGBClassifier(colsample_by_tree=0.1,\n",
    "                                  learning_rate=0.89,\n",
    "                                  max_depth=8,\n",
    "                               n_estimators=10000,\n",
    "                                  eval_metric=\"auc\",                                \n",
    "                                  n_jobs=-1, silent=0, verbose=0),\n",
    "    \"MLPClassifier\" : MLPClassifier(hidden_layer_sizes=(10), solver = \"lbfgs\", max_iter=1000, random_state=42),\n",
    "    \"LinearRegression\" : LinearRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/2_Semester/Machine Learning/MLProjects/Proposal/ML_algorithms.py:66: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  model.add(layers.Dense(1, activation=\"sigmoid\", init=init))\n"
     ]
    }
   ],
   "source": [
    "models = {\"KerasNN_not_fitted\" : KerasNN_not_fitted(input_dim=39)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KerasNN_not_fitted': 0.412}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.Cross_Val_Models(models, X, y, sampling_technique=None, scaler=scaler)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy import stats\n",
    "\n",
    "def Binning_Features(df, feature=\"Income\", n_bins=10, strategy=\"quantile\", cont_tab=False):\n",
    "    target = \"Response\"\n",
    "\n",
    "    bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "    mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "\n",
    "    if cont_tab == True:\n",
    "        obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "        df[feature] = mnt_bin\n",
    "        print(obs_cont_tab)\n",
    "        \n",
    "    df[feature] = mnt_bin\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\"MntWines\",\"MntFruits\",\"Income\",'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
    "       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
    "       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "\n",
    "for feature in features:\n",
    "    Binning_features(df, feature=\"Income\", n_bins=10, cont_tab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Income_bin\"] = mnt_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cont_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) obtain the observed contingency table\n",
    "feature, target = \"Education\", \"Response\"\n",
    "df_rec = df[[feature, target]]\n",
    "\n",
    "obs_cont_tab = pd.crosstab(df_rec[feature], df_rec[target], margins = True)\n",
    "obs_cont_tab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) calculate the expected contingency table, assuming there is no association\n",
    "exp_cont_tab = obs_cont_tab.copy()\n",
    "\n",
    "n_r = exp_cont_tab.iloc[:-1, -1]\n",
    "N = exp_cont_tab.iloc[-1, -1]\n",
    "for c in range(obs_cont_tab.shape[1]-1):\n",
    "    n_c = exp_cont_tab.iloc[-1, c]\n",
    "    exp_cont_tab.iloc[:-1, c] = np.divide(np.multiply(n_c, n_r), N)\n",
    "\n",
    "# visually compare both tables, side by side\n",
    "pd.concat([obs_cont_tab.iloc[:, :-1], exp_cont_tab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) compute the test statistic as measure of dissimilarity between the expected and observed tables\n",
    "obs, exp = [], []\n",
    "for c in range(obs_cont_tab.shape[1]-1):\n",
    "    exp.append(exp_cont_tab.iloc[:-1, c].values)\n",
    "    obs.append(obs_cont_tab.iloc[:-1, c].values)\n",
    "    \n",
    "chi_squared_stat = np.sum(np.divide(np.power(np.subtract(obs, exp), 2), exp))\n",
    "print('Chi-squared test statistic: {0:.2f}'.format(chi_squared_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "chisq = stats.chi2_contingency(obs_cont_tab.iloc[:-1, [0, 1]].values)[0:2]\n",
    "print(\"Test statistic: {0:.2f}, p-value: {1:.4f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bar_charts_categorical(df, feature, target):\n",
    "    cont_tab = pd.crosstab(df[feature], df[target], margins = True)\n",
    "    categories = cont_tab.index[:-1]\n",
    "        \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, \n",
    "                 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, \n",
    "                 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"red\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "    \n",
    "    mean_target = 1-df[target].mean()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"red\")\n",
    "    plt.plot([.5, len(categories)+.5], [mean_target, mean_target],'--', lw=2, color=\"black\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i in df:\n",
    "    bar_charts_categorical(df, i, \"Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# 1) equal width binning of a continuous feature\n",
    "feature, n_bins = \"Income\", 10\n",
    "bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "print(\"Parameters of discretizer: \", bindisc.get_params())\n",
    "print(\"Thresholds for {} bins of {}: \".format(n_bins, feature), bindisc.bin_edges_)\n",
    "\n",
    "# 2) generate a contingency table, required for the Chi-Squared test\n",
    "obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "\n",
    "# 3) compute Chi-Squared test for binned feature\n",
    "chisq=stats.chi2_contingency(obs_cont_tab.values)[0:2]\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) equal width binning of a continuous feature\n",
    "feature, n_bins = \"Income\", 10\n",
    "bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "mnt_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "mnt_bin = pd.Series(mnt_bin[:, 0], index=df.index)\n",
    "print(\"Parameters of discretizer: \", bindisc.get_params())\n",
    "print(\"Thresholds for {} bins of {}: \".format(n_bins, feature), bindisc.bin_edges_)\n",
    "\n",
    "# 2) generate a contingency table, required for the Chi-Squared test\n",
    "obs_cont_tab = pd.crosstab(mnt_bin, df[target])\n",
    "\n",
    "# 3) compute Chi-Squared test for binned feature\n",
    "chisq=stats.chi2_contingency(obs_cont_tab.values)[0:2]\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chisq[0], chisq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bar_charts_continuous(df, feature, target, n_bins=10, binning_strategy=\"uniform\", chi_sq=False):\n",
    "    bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n",
    "                               strategy=binning_strategy)\n",
    "    feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "    feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n",
    "    \n",
    "    cont_tab = pd.crosstab(feature_bin, df[target], margins = True)\n",
    "    categories = cont_tab.index[:-1].astype(str) \n",
    "    \n",
    "    if chi_sq:\n",
    "        chisq = stats.chi2_contingency(cont_tab.values)[0:2]       \n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    p1 = plt.bar(categories, cont_tab.iloc[:-1, 0].values, \n",
    "                 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, cont_tab.iloc[:-1, 1].values, \n",
    "                 0.55, bottom=cont_tab.iloc[:-1, 0], color=\"red\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Frequency bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$Frequency$\")\n",
    "\n",
    "    # auxiliary data for 122\n",
    "    obs_pct = np.array([np.divide(cont_tab.iloc[:-1, 0].values, cont_tab.iloc[:-1, 2].values), \n",
    "                        np.divide(cont_tab.iloc[:-1, 1].values, cont_tab.iloc[:-1, 2].values)])\n",
    "    \n",
    "    mean_target = 1-df[target].mean()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    p1 = plt.bar(categories, obs_pct[0], 0.55, color=\"gray\")\n",
    "    p2 = plt.bar(categories, obs_pct[1], 0.55, bottom=obs_pct[0], color=\"red\")\n",
    "    plt.plot([-.5, len(categories)], [mean_target, mean_target],'--', lw=2, color=\"black\")\n",
    "    plt.legend((p2[0], p1[0]), ('$y_i=1$', '$y_i=0$'))\n",
    "    plt.title(\"Proportion bar chart\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"$p$\")\n",
    "    plt.show()\n",
    "    \n",
    "    if chi_sq:\n",
    "        return chisq\n",
    "\n",
    "chi_sq=bar_charts_continuous(df, \"Income\", \"Response\", 10, \"uniform\", True)\n",
    "print(\"Test statistic: {0:0.2f}, p-value: {1:0.2f}\".format(chi_sq[0], chi_sq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, target = \"Income\", \"Response\"\n",
    "df_smd = df[[feature, target]]\n",
    "\n",
    "mean_feature = df_smd[feature].mean()\n",
    "df_smd[feature+\"_SMD\"] = (df_smd[feature].sub(mean_feature)).div(mean_feature)\n",
    "\n",
    "smd_summary = df_smd.groupby([target]).agg({feature+\"_SMD\": \"mean\", feature: \"mean\"})  \n",
    "display(smd_summary)\n",
    "print(\"Average {0}: {1:.2f}, SMD of {2}: {3:.2f}\".format(feature, mean_feature, feature, smd_summary[feature+\"_SMD\"].abs().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the bar chart\n",
    "plt.bar(x = smd_summary.index, height=smd_summary[feature+\"_SMD\"], color=\"gray\")\n",
    "plt.title(\"SMD of \" + feature)\n",
    "plt.xlabel(target)\n",
    "plt.ylabel(\"$SMD$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) intermediary step: compute SMD0, SMD1 and SMD for every continuous feature\n",
    "num_list = df.select_dtypes(include=[\"number\"]).drop([\"Response\"], axis=1).columns\n",
    "target = \"Response\"\n",
    "\n",
    "smd_dict = {\"smd0\": [], \"smd1\": [], \"smd\": []}\n",
    "for feature in num_list:\n",
    "    df_smd = df[[feature, target]]\n",
    "    mean_feature = df_smd[feature].mean()\n",
    "    df_smd[feature] = (df_smd[feature].sub(mean_feature)).div(mean_feature) \n",
    "    smd_summary = df_smd.groupby([target]).agg({feature: \"mean\"})  \n",
    "    smd_dict[\"smd0\"].append(smd_summary.iloc[0, 0])\n",
    "    smd_dict[\"smd1\"].append(smd_summary.iloc[1, 0])\n",
    "    smd_dict[\"smd\"].append(smd_summary[feature].abs().sum())\n",
    "\n",
    "smd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) intermediary step: plot SMD0 (gray), SMD1 (red) ordered by SMD\n",
    "import operator\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.DataFrame(smd_dict, index=num_list).sort_values(\"smd\", ascending=False)\n",
    "colors, groups = (\"gray\", \"red\"), (\"non-respondents\", \"respondents\")\n",
    "g0, g1 = (df1.index, df1.iloc[:, 0]), (df1.index, df1.iloc[:, 1])\n",
    "data = (g0, g1)\n",
    "\n",
    "fig = plt.figure(figsize=(len(df1.index)+1, 4))\n",
    "ax = fig.add_subplot(1, 1, 1, facecolor = \"1.0\")\n",
    "for data, color, group in zip(data, colors, groups):    \n",
    "    X, y = data\n",
    "    ax.scatter(x=X, y=y.values, s=80, marker=\"s\", label=group, c=color)\n",
    "\n",
    "ax.set_title(\"Scaled Mean Deviation (SMD)\")\n",
    "ax.set_xlabel(\"Continuous features\")\n",
    "ax.set_ylabel(\"SMD\")\n",
    "ax.legend()\n",
    "plt.xticks(fontsize=9, rotation=60)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smd01_plot(df, nfeature_list, target, return_df_smd=True):  \n",
    "    smd_dict = {\"smd0\": [], \"smd1\": [], \"smd\": []}\n",
    "    for feature in num_list:\n",
    "        df_smd = df[[feature, target]]\n",
    "        mean_feature = df_smd[feature].mean()\n",
    "        df_smd[feature] = (df_smd[feature].sub(mean_feature)).div(mean_feature) \n",
    "        df_smd = df_smd.groupby([target]).agg({feature: \"mean\"})  \n",
    "        smd_dict[\"smd0\"].append(df_smd.iloc[0, 0])\n",
    "        smd_dict[\"smd1\"].append(df_smd.iloc[1, 0])\n",
    "        smd_dict[\"smd\"].append(df_smd[feature].abs().sum())\n",
    "\n",
    "    df_smd = pd.DataFrame(smd_dict, index=num_list).sort_values(\"smd\", ascending=False)\n",
    "    colors, groups = (\"gray\", \"red\"), (\"non-respondents\", \"respondents\")\n",
    "    g0, g1 = (df_smd.index, df_smd.iloc[:, 0]), (df_smd.index, df_smd.iloc[:, 1])\n",
    "    data = (g0, g1)\n",
    "\n",
    "    fig = plt.figure(figsize=(len(df_smd.index)+1, 4))\n",
    "    ax = fig.add_subplot(1, 1, 1, facecolor = \"1.0\")\n",
    "    for data, color, group in zip(data, colors, groups):    \n",
    "        X, y = data\n",
    "        ax.scatter(x=X, y=y.values, s=80, marker=\"s\", label=group, c=color)\n",
    "\n",
    "    ax.set_title(\"Scaled Mean Deviation (SMD)\")\n",
    "    ax.set_xlabel(\"Continuous features\")\n",
    "    ax.set_ylabel(\"SMD\")\n",
    "    ax.legend()\n",
    "    plt.xticks(fontsize=9, rotation=60)    \n",
    "    plt.show()\n",
    "    \n",
    "    if return_df_smd:\n",
    "        return df_smd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smd = smd01_plot(df, num_list, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def chisq_ranker(df, target,continuous_flist = list(df.select_dtypes(include=[\"number\"]).drop([\"Response\"], axis=1).columns),categorical_flist = list(df.select_dtypes(include=[\"object\"]).columns), n_bins=10, binning_strategy=\"uniform\"):\n",
    "    \"\"\"\n",
    "    Input the dataframe and the target, you can also choose the continuous variables you want to include from the dataframe but a default has been set.\n",
    "    The same goes for the categorical variables.\n",
    "    The output that will be given is the ranking of the features using the Chi-Squared test\n",
    "    \"\"\"\n",
    "    chisq_dict = {}\n",
    "    if  continuous_flist:\n",
    "        bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n",
    "                               strategy=binning_strategy)\n",
    "        for feature in continuous_flist:            \n",
    "            feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n",
    "            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n",
    "            cont_tab = pd.crosstab(feature_bin, df[target], margins = False)\n",
    "            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2] \n",
    "    if  categorical_flist:\n",
    "        for feature in categorical_flist:  \n",
    "            cont_tab = pd.crosstab(df[feature], df[target], margins = False)          \n",
    "            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2]\n",
    "            \n",
    "    df_chisq_rank = pd.DataFrame(chisq_rank, index=[\"Chi-Squared\", \"p-value\"]).transpose()\n",
    "    df_chisq_rank.sort_values(\"Chi-Squared\", ascending=False, inplace=True)\n",
    "    df_chisq_rank[\"valid\"]=df_chisq_rank[\"p-value\"]<=0.05\n",
    "    df_chisq_rank\n",
    "    return df_chisq_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chisq_rank = chisq_ranker(df, target)\n",
    "df_chisq_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisq_ranker_plot(df_chisq_rank):\n",
    "    \"\"\"\n",
    "    The Chi-Square ranker plot takes the output of the Chi-Squared ranker function and creates a plot out of the inputted dataframe.\n",
    "    \"\"\"\n",
    "    colors = {True: 'gray', False: 'lightgray'}\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    df_chisq_rank['Chi-Squared'].plot(ax=ax, kind='bar', color=[colors[i] for i in df_chisq_rank['valid']])\n",
    "    ax.set_title(\"Features worth by Chi-Squared test statistic\")\n",
    "    ax.set_xlabel(\"Input features\")\n",
    "    ax.set_ylabel(\"Test statistic\")\n",
    "    ax.set_xticklabels(df_chisq_rank.index, rotation=70)\n",
    "    ax.legend([\"p-value<=0.5\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq_ranker_plot(df_chisq_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_income_KNN(df):\n",
    "    dataframe = df.copy()\n",
    "    dataframe_c = dataframe.dropna().select_dtypes(include=[\"number\"]).drop([\"Response\"], axis = 1)\n",
    "    dataframe_i = dataframe[pd.isnull(dataframe).any(axis=1)].select_dtypes(include=[\"number\"]).drop([\"Response\"], axis = 1)\n",
    "    clf = KNeighborsClassifier(3, weights='uniform', metric = 'euclidean')\n",
    "    trained_model = clf.fit(dataframe_c.drop([\"Income\"],axis=1), dataframe_c.loc[:,'Income'])\n",
    "    imputed_values = trained_model.predict(dataframe_i.drop([\"Income\"], axis=1))\n",
    "    #print(imputed_values)\n",
    "    dataframe_i[\"Income\"] = imputed_values\n",
    "    dataframe_new = pd.concat([df_i, df_c])\n",
    "    dataframe_new = dataframe_new.sort_index()\n",
    "    dataframe[\"Income\"] = dataframe_new[\"Income\"]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_IQR(df, columns=[\"Year_Birth\",\"Income\"]):\n",
    "    \"\"\"\n",
    "    outlier deletetion using the IQR, you can choose the variables you want to delete the outliers for by selecting the columns. The default is Year_Birth & Income. Also you can change the quantile values.\n",
    "    \"\"\"\n",
    "    dataframe = df.copy()\n",
    "    Q1 = dataframe[columns].quantile(0.25)\n",
    "    Q3 = dataframe[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    #print(IQR)\n",
    "    dataframe[columns] = dataframe[columns][~((dataframe < (Q1 - 2 * IQR)) |(dataframe > (Q3 + 2 * IQR))).any(axis=1)]\n",
    "    dataframe = dataframe.dropna()\n",
    "    print('Removing outliers using the IQR method with 2 quartiles would lead to a change of data size: ',(dataframe.shape[0] -df.shape[0]) /df.shape[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_ZSCORE(df, columns=[\"Year_Birth\", \"Income\"], threshold=3):\n",
    "    \"\"\"\n",
    "    outlier deletion using the Zscore, you can choose which columns you want to apply it on and you can choose which threshold you want to use.\n",
    "    \"\"\"\n",
    "    dataframe = df.copy()\n",
    "    columns_zscore = []\n",
    "    for i in dataframe[columns]:\n",
    "        i_zscore = i + \"_zscore\"\n",
    "        columns_zscore.append(str(i_zscore))\n",
    "        \n",
    "        dataframe[i_zscore] = (dataframe[i] - dataframe[i].mean())/df[i].std(ddof=0)\n",
    "    for i in dataframe[columns_zscore]:\n",
    "        dataframe = dataframe[(dataframe[i] < threshold) & (dataframe[i] > -threshold)]\n",
    "    dataframe = dataframe.drop(columns_zscore, axis=1)\n",
    "    print('Removing outliers using the ZSCORES method with a threshold of 3 would lead to a change of data size: ',(dataframe.shape[0] -df.shape[0]) /df.shape[0])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_IQR(df,[\"Year_Birth\",\"Income\",\"MntSweetProducts\",\"MntMeatProducts\",\"MntGoldProds\"]).head()\n",
    "\n",
    "outlier_ZSCORE(df,[\"Year_Birth\",\"Income\",\"MntSweetProducts\",\"MntMeatProducts\",\"MntGoldProds\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def Min_Max_Train(X_train, X_test):    \n",
    "    scaler = MinMaxScaler()\n",
    "    # Only fit the training data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def KerasNN1(X_train, X_test, y_train, y_test, n_layers=4, optimizer=\"rmsprop\", loss=\"binary_crossentropy\", init=\"uniform\", metrics=[\"accuracy\"]):\n",
    "    \"\"\"\n",
    "    Keras Neural Network, define the amount of layers you want, which optimizer you want to use and which loss function you want to apply.\n",
    "    \"\"\" \n",
    "    np.random.seed(42)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(6, activation=\"relu\", input_dim=len(df.columns) -1))\n",
    "    for num in range(n_layers-2):\n",
    "        model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\", init=init))\n",
    "    model.compile(optimizer, loss, metrics=metrics)\n",
    "    \n",
    "    initial_weights = model.get_weights()\n",
    "    \n",
    "    shuffle_weights(model, initial_weights)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_evaluation(model):\n",
    "    y_predicted = model.predict(X_test)\n",
    "    threshold = utils.max_threshold(y_predicted, y_test, threshold_range = (0.1, 0.99),iterations=10000, visualization=True)\n",
    "    y_pred = utils.predict_with_threshold(y_predicted,threshold)\n",
    "\n",
    "    print(\"Accuracy {:1.2f}\".format(utils.calculate_accuracy(y_pred, y_test)))\n",
    "    print(\"Area under the curve {:1.2f}\".format(utils.calculate_auc(y_pred, y_test)))\n",
    "    print(\"Precision {:1.2f}\".format(utils.calculate_precision_score(y_pred, y_test)))\n",
    "    print(\"Recall {:1.2f}\".format(utils.calculate_recall_score(y_pred, y_test)))\n",
    "    print(\"Profit Share {:1.2f}\".format(utils.profit_share(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def KerasNN(X_train, X_test, y_train, y_test, optimizer=\"rmsprop\", loss=\"binary_crossentropy\"):\n",
    "    \"\"\"\n",
    "    Try to variate in optimizers & loss functions\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    # Only fit the training data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # if we can pass a variable that indicates the amount of layers than it would be cool\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(6, activation=\"relu\", input_dim=30))\n",
    "    model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer, loss, metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    threshold = utils.max_threshold(y_predicted, y_test, threshold_range = (0.1, 0.99),iterations=10000, visualization=True)\n",
    "    y_pred = utils.predict_with_threshold(y_predicted,threshold)\n",
    "    \n",
    "    print(\"Accuracy {:1.2f}\".format(utils.calculate_accuracy(y_pred, y_test)))\n",
    "    print(\"Area under the curve {:1.2f}\".format(utils.calculate_auc(y_pred, y_test)))\n",
    "    print(\"Precision {:1.2f}\".format(utils.calculate_precision_score(y_pred, y_test)))\n",
    "    print(\"Recall {:1.2f}\".format(utils.calculate_recall_score(y_pred, y_test)))\n",
    "    print(\"Profit Share {:1.2f}\".format(utils.profit_share(y_pred, y_test)))\n",
    "    return utils.profit_share(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
