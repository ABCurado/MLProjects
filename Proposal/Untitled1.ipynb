{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils\n",
    "import preprocessing\n",
    "#import data_visualization\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "import feature_engineering\n",
    "from ML_algorithms import *\n",
    "import pandas as pd\n",
    "#from seaborn import countplot\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [\n",
    "    (\"StandardScaler\", StandardScaler()),\n",
    "    #(\"RobustScaler\", RobustScaler()),\n",
    "    (\"MinMaxScaler\", MinMaxScaler()),\n",
    "    (\"None\", None)\n",
    "]\n",
    "\n",
    "samplers =  [\n",
    "    (\"RandomOverSampler\", RandomOverSampler(random_state=42, ratio=0.5)),\n",
    "    #(\"TomekLinks\", TomekLinks(random_state=42)),\n",
    "    #(\"EditedNN\", EditedNearestNeighbours(random_state=42, n_neighbors=3)),\n",
    "    #(\"SMOTE\", SMOTE(random_state=42, ratio=0.5)),\n",
    "    #(\"SMOTETomek\",SMOTETomek(random_state=42, ratio=0.8))\n",
    "    (\"None\", None)\n",
    "    \n",
    "]\n",
    "\n",
    "pre_processing_pipelines = [\n",
    "    (\"Joris_Pipeline\", preprocessing.joris_preprocessing_pipeline),\n",
    "    (\"Morten_Pipeline\", preprocessing.morten_preprocessing_pipeline),\n",
    "    (\"Bin it!\", preprocessing.bin_it_preprocessing_pipeline)\n",
    "\n",
    "]\n",
    "seed = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.get_dataset()\n",
    "df = pre_processing_pipeline[1](df)\n",
    "X, y = utils.X_y_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_Val_Models(models, X, y, scaler=None, n_splits=5, sampling_technique=None):\n",
    "    \"\"\"\n",
    "    Pass the dictionary of all the model you want to do the cross validation for. \n",
    "    For Example:  {\"GaussianNB\" : GaussianNB(), \"MultinomialNB\" : MultinomialNB()}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for model in models.keys():\n",
    "        y_predicted = cross_validation_average_results(models[model], X, y, n_splits,scaler=scaler,sampling_technique=sampling_technique)\n",
    "        threshold = max_threshold(y_predicted, y, threshold_range = (0.1, 0.99),iterations=1000, visualization=True)\n",
    "        y_pred = predict_with_threshold(y_predicted,threshold)\n",
    "        results[model] = profit_share(y_pred, y)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
